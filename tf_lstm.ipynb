{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(1337)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and scramble its rows\n",
    "data = pd.read_csv('./imdb_processed.csv')\n",
    "data = data.iloc[np.random.permutation(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [07:25<?, ?it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 58871.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to txt\n"
     ]
    }
   ],
   "source": [
    "# TODO: SHOULD DO THIS ONLY ON THE TRAINING DATA\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# # get all processed reviews\n",
    "# reviews = data.text.values\n",
    "# # merge into single variable, separated by whitespaces\n",
    "# words = ' '.join(reviews)\n",
    "# # obtain list of words\n",
    "# words = words.split()\n",
    "\n",
    "words = data['text'].str.cat(sep=' ').split()\n",
    "\n",
    "# build vocabulary\n",
    "frequency_counter = Counter(words)\n",
    "# sort words by the frequency they appear in the text\n",
    "vocab = sorted(frequency_counter, key=frequency_counter.get, reverse=True)\n",
    "\n",
    "# associate a number to each word in the list in ascending order\n",
    "# in this way the most frequent words have lower numbers\n",
    "int2word = dict(enumerate(vocab[:5000], 2))\n",
    "int2word[0] = '<PAD>'\n",
    "int2word[1] = '<UNK>'\n",
    "word2int = {word: id for id, word in int2word.items()}\n",
    "# encode words\n",
    "reviews_enc = [[word2int.get(word, '<UNK>') for word in review.split()] for review in tqdm(data.text.values)]\n",
    "\n",
    "with open('tmp.txt', 'w') as file:\n",
    "  for el in reviews_enc:\n",
    "    file.write(str(el) + '\\n')  # Convert the list to a string and add a newline character\n",
    "\n",
    "print(\"saved to txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "validation_size = 0.5\n",
    "\n",
    "split_id = int(len(data) * train_size)\n",
    "\n",
    "features = reviews_enc\n",
    "labels = data['label']\n",
    "\n",
    "temp_train_x, test_x = features[:split_id], features[split_id:]\n",
    "temp_train_y, test_y = labels[:split_id], labels[split_id:]\n",
    "\n",
    "# make val and test set\n",
    "split_val_id = int(len(temp_train_x) * validation_size)\n",
    "train_x, val_x = temp_train_x[:split_val_id], temp_train_x[split_val_id:]\n",
    "train_y, val_y = temp_train_y[:split_val_id], temp_train_y[split_val_id:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(train_x, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(test_x, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(counter))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, train_y, validation_data=(X_test, test_y), epochs=3, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
