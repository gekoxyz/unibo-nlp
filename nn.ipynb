{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    50000.000000\n",
      "mean       858.389040\n",
      "std        658.428061\n",
      "min         22.000000\n",
      "25%        452.000000\n",
      "50%        633.000000\n",
      "75%       1044.000000\n",
      "max       9434.000000\n",
      "Name: text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "# read processed data\n",
    "data = pd.read_csv('./imdb_processed.csv')\n",
    "data = data.iloc[np.random.permutation(len(data))]\n",
    "print(data['text'].str.len().describe())\n",
    "\n",
    "data['text'] = data['text'].str.slice(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shapes:\n",
      "===============\n",
      "Test set: (10000,)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "validation_size = 0.5\n",
    "\n",
    "split_id = int(len(data) * train_size)\n",
    "\n",
    "temp_train_x, test_x = data.text[:split_id], data.text[split_id:]\n",
    "temp_train_y, test_y = data.label[:split_id], data.label[split_id:]\n",
    "\n",
    "# now we have train, val and test\n",
    "print('Feature Shapes:')\n",
    "print('===============')\n",
    "# print('Train set: {}'.format(train_x.shape))\n",
    "# print('Validation set: {}'.format(val_x.shape))\n",
    "print('Test set: {}'.format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 92114.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: SHOULD DO THIS ONLY ON THE TRAINING DATA\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "words = temp_train_x.str.cat(sep=' ').split()\n",
    "\n",
    "# build vocabulary\n",
    "frequency_counter = Counter(words)\n",
    "# sort words by the frequency they appear in the text\n",
    "vocab = sorted(frequency_counter, key=frequency_counter.get, reverse=True)\n",
    "\n",
    "# associate a number to each word in the list in ascending order\n",
    "# in this way the most frequent words have lower numbers\n",
    "int2word = dict(enumerate(vocab[:5000], 2))\n",
    "int2word[0] = '<PAD>'\n",
    "int2word[1] = '<UNK>'\n",
    "word2int = {word: id for id, word in int2word.items()}\n",
    "# encode words\n",
    "reviews_enc = [[word2int.get(word, 1) for word in review.split()] for review in tqdm(temp_train_x.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe float 16\n",
    "features = np.zeros((len(reviews_enc), 1000), dtype=int)\n",
    "\n",
    "for i, row in enumerate(reviews_enc):\n",
    "  # print(f\"ROW LENGTH {len(row)}\")\n",
    "  # print(f\"ITEM {i} COULD BE INSERTED AT {1000 - len(row)}\")\n",
    "  # print(f\"######################\")\n",
    "  index = 1000 - len(row)\n",
    "  features[i, index:] = np.array(row)[:1000]\n",
    "  # features[i, :len(row)] = np.array(row)[:1000]\n",
    "\n",
    "# make val and test set\n",
    "split_val_id = int(len(temp_train_x) * validation_size)\n",
    "train_x, val_x = features[:split_val_id], features[split_val_id:]\n",
    "train_y, val_y = temp_train_y[:split_val_id], temp_train_y[split_val_id:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# define batch size\n",
    "batch_size = 128\n",
    "\n",
    "# create tensor datasets\n",
    "trainset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y.to_numpy()))\n",
    "validset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y.to_numpy()))\n",
    "# testset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# create dataloaders\n",
    "trainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
    "valloader = DataLoader(validset, shuffle=True, batch_size=batch_size)\n",
    "# testloader = DataLoader(testset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "  def __init__(self, vocab_size, output_size, hidden_size=128, embedding_size=400, n_layers=2, dropout=0.2):\n",
    "    super(SentimentModel, self).__init__()\n",
    "    # embedding layer is useful to map input into vector representation\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "    # LSTM layer preserved by PyTorch library\n",
    "    self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
    "    # dropout layer\n",
    "    self.dropout = nn.Dropout(0.3)\n",
    "    # Linear layer for output\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "    # Sigmoid layer cz we will have binary classification\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # convert feature to long\n",
    "    x = x.long()\n",
    "    # map input to vector\n",
    "    x = self.embedding(x)\n",
    "    # pass forward to lstm\n",
    "    o, _ =  self.lstm(x)\n",
    "    # get last sequence output\n",
    "    o = o[:, -1, :]\n",
    "    # apply dropout and fully connected layer\n",
    "    o = self.dropout(o)\n",
    "    o = self.fc(o)\n",
    "    # sigmoid\n",
    "    o = self.sigmoid(o)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# define training device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE: 5002\n",
      "SentimentModel(\n",
      "  (embedding): Embedding(5002, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.25)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model hyperparamters\n",
    "vocab_size = len(word2int)\n",
    "output_size = 1\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "n_layers = 2\n",
    "dropout=0.25\n",
    "\n",
    "print(f\"VOCAB SIZE: {vocab_size}\")\n",
    "\n",
    "# model initialization\n",
    "model = SentimentModel(vocab_size, output_size, hidden_size, embedding_size, n_layers, dropout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config\n",
    "lr = 0.001\n",
    "criterion = nn.BCELoss()  # we use BCELoss cz we have binary classification problem\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "grad_clip = 5\n",
    "epochs = 8\n",
    "print_every = 1\n",
    "history = {\n",
    "  'train_loss': [],\n",
    "  'train_acc': [],\n",
    "  'val_loss': [],\n",
    "  'val_acc': [],\n",
    "  'epochs': epochs\n",
    "}\n",
    "es_limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: read and understand training and testing data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
