\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\graphicspath{ {./images/} }

\title{Sentiment analysis su dataset IMDB}

\author{
 Matteo Galiazzo \\
  Dipartimento di Informatica - Scienza e Ingegneria\\
  Universit√† di Bologna\\
  \texttt{matteo.galiazzo@studio.unibo.it} \\
  %% examples of more authors
%    \And
%  Zixuan Lu \\
%   School of Coumputing and Information\\
%   University of Pittsburgh\\
%   Pittsburgh, PA 15213 \\
%   \texttt{ZIL50@pitt.edu} \\
%   \And
%  Yuchen Lu \\
%   School of Coumputing and Information\\
%   University of Pittsburgh\\
%   Pittsburgh, PA 15213 \\
%   \texttt{yul217@pitt.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle
\begin{abstract}
This project presents an implementation of sentiment analysis on the IMDB movie reviews dataset using deep learning approaches.
The project explores two different Long Short-Term Memory (LSTM) architectures: a standard LSTM and a bidirectional LSTM model.
The implementation leverages TensorFlow and Keras for model development and training, with the goal of accurately classifying movie reviews as either positive or negative.
\end{abstract}

% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}


\subsection{Recurrent Neural Networks}

The problem of using a MLP or CNN to process text is the fact that we have a fixed input.
This means that if we use one of this architectures we are constrained by a fixed window for our input.
For solving this problem scholars introduced Recurrent Neural Networks (RNNs).
In a RNN we have a sequence of words $x_1, ..., x_n$.
To process this sequence we inject $x_1$ in the hidden layer and output $o_1$.
The information of $x_2$ is processed together with the output of the previous computation.
So in principle when you compute the output of $x_n$ you take into consideration all the previous output.
This way the sequence length is independent from the network structure.

Two very common memory cells for RNNs are Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU).
RNNs have a problem. They can only see the past of the sequence.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{img/rnn_unfolded.png}
  \caption{A compressed (left) and unfolded (right) RNN}
  \label{fig:rnn_unfolded}
\end{figure}

Abstractly speaking, an RNN is a function $f_{\theta}$ of type $(x_t, h_t) \rightarrow (y_t, h_{t+1})$, where:
\begin{itemize}
  \item $x_t$: input vector.
  \item $h_t$: hidden vector.
  \item $y_t$: output vector.
  \item $\theta$: neural network parameters.
\end{itemize}

It's a neural netowrk that maps an input $x_t$ into an output $y_t$, with the hidden vector $h_t$ playing the role of "memory", a partial record of all previous input-output pairs.
At each step, it transforms input to an output, and modifies its "memory" to help it to better perform future processing.

The figure \ref{fig:rnn_unfolded} may be misleading to many because practical neural netowrk topologies are frequently organized in "layers" and the drawing gives that appearance.
However, what appears to be layers are, in fact, different steps in time, "unfolded" to produce the appearance of layers.

\subsubsection{Bidirectional RNNs}

We can combine two RNNs, one processing the input sequence in one direction, and the other one in the opposite direction
This is called a bidirectional RNN, and it's structured as follows:
\begin{itemize}
  \item The forward RNN processes in one direction: $f_{\theta}(x_0, h_0) = (y_0, h_1), f_{\theta}(x_1, h_1) = (y_1, h_2), ...$
  \item The backward RNN processes in the opposite direction:
  
  ${f'}_{\theta '}(x_N, {h'}_N) = ({y'}_N, {h'}_{N-1}), {f'}_{\theta '}(x_{N-1}, {h'}_{N-1}) = ({y'}_{N-1},{h'}_{N-2}), ...$
\end{itemize}

The two output sequences are then concatenated to give the total output: $((y_0, {y'}_0), (y_1, {y'}_1), ..., (y_N, {y'}_N))$.
Bidirectional RNN allows the model to process a token both in the context of what came before it and what came after it.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{img/rnn_bidirectional.png}
  \caption{An unfolded bidirectional RNN}
  \label{fig:rnn_bidirectional}
\end{figure}

\subsubsection{LSTM}
\label{sec:lstm}

While Recurrent Neural Networks (RNNs) provide a mechanism for processing sequential data by maintaining a hidden state that captures information from previous steps, they suffer from the vanishing gradient problem.
This issue happens when gradients used to update the network's parameters during training diminish exponentially over time, making it difficult for the network to learn long term dependencies.
To adress this, Long Short-Term Memory (LSTM) networks were introduced as a specialized variant of RNNs.
LSTMs can better capture long-term dependencies by incorporating a more sophisticated memory mechanism and gating structure.

In LSTMs the module, instead of having a single neural network layer, has four parts interacting in a special way.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{img/lstm_chain.png}
  \caption{The repeating module in an LSTM}
  \label{fig:lstm_chain}
\end{figure}

In the diagram in figure \ref{fig:lstm_chain}, each line carries an entire vector, from the output of one node to the inputs of others.
The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers.
Lines merging denote concatenation, while a line fork denotes its content being copied and the copies going to different locations.

\paragraph{The core idea behind LSTMs}

The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.
The cell state is like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions.
It's very easy for information to just flow along it unchanged.

The LSTM does have the ability to remove or add information to the cell state, carefully regulated bu structures called gates.
Gates are a way to optionally let information through.
They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.

The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means "let nothing through", while a value of one means "let everything through".
An LSTM has three of these gates, to protect and control the cell state.

\paragraph{The four LSTM parts}
The first step in our LSTM is to decide what information we're going to throw away from the cell state.
This decision is made by a sigmoid layer called the "forget gate layer".
It looks at $h_{t-1}$ and $x_t$ and outputs a number between 0 and 1 for each number in the cell state $C_{t-1}$.
A 1 represents "remember this" while a 0 represents "forget this".

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{img/lstm_forget.png}
  \caption{The forget module in an LSTM}
  \label{fig:lstm_forget}
\end{figure}

The next step is to decide what new information we're going to store in the cell state.
This has two parts.
First a sigmoid layer called the "input gate layer" decides which values we'll update.
Next, a tanh layer creates a vector of new candidate values, $\tilde{C}_t$, that could be added to the state.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{img/lstm_input.png}
  \caption{The input module in an LSTM}
  \label{fig:lstm_input}
\end{figure}

Next, we have to update the old cell state $C_{t-1}$, into the new cell state $C_t$.
The previous steps already decided what to do, we just need to actually do it.
We multiply the old state by $f_t$, forgetting the things we decided to forget earlier.
Then we add $i_t * \tilde{C}_t$.
This is the new candidate values, scaled by how much we decided to update each state value.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{img/lstm_update.png}
  \caption{The update module in an LSTM}
  \label{fig:lstm_update}
\end{figure}

Finally, we need to decide what we're going to output.
This output will be based on our cell state, but will be a filtered version.
First, we run a sigmoid layer which decides what parts of the cell sate we're going to output.
Then, we put the cell state through tanh (to push the values between -1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{img/lstm_output.png}
  \caption{The output module in an LSTM}
  \label{fig:lstm_output}
\end{figure}



\section{Technical implementation}

\subsection{Dataset}
IMDB is a popular website that collects movie reviews.
The IMDB dataset is a widely used benchmark for sentiment analysis tasks;
it contains 50000 movie reviews from the website, allowing no more than 30 reviews per movie.
The dataset contains an even number of positive and negative reviews, so randomly guessing yields 50\% accuracy.
Only highly polarized reviews are considered.
A negative review has a score $\leq 4$ out of 10, and a positive review has a score $\geq 7$ out of 10. Neutral reviews are not included in the dataset. \cite{imdb_dataset_stanfordnlp}. 

\subsection{Data preprocessing}

The implementation includes several preprocessing steps.
The first one is data preprocessing.
Data preprocessing is important because raw textual data often contains noise, suck as typographical errors, inconsistent formatting and irrelevant symbols.
Preprocessing techniques aim to transform raw text data into a clean format that facilitates effective feature extraction and model training.

In this case I cleaned the dataset by removing: links, html tags, double spaces, punctuation and stopwords.
Stopwords are high frequency and low information words such as articles, prepositions, and conjunctions (e.g. "the", "and", "in") that are excluded to reduce noise and improve computational efficiency.
The stopwords were removed by using NLTK (Natural Language ToolKit), a suite of libraries and programs for natural language processing written in Python.
I also lemmatized the dataset.
Lemmatization is the process of reducing words to their root form, known as a lemma.
Unlike stemming, which simply chops off word endings, lemmatization considers the context and part of speech of a word to return a valid dictionary form (e.g. "running" is "run" and "better" is "good").
The entire dataset was also converted to lowercase.

We can see the dataset's head in the following code snippet:
\begin{itemize}
  \item Before the preprocessing:
    \begin{lstlisting}
    0      I rented I AM CURIOUS-YELLOW from my video sto...      0
    1      "I Am Curious: Yellow" is a risible and preten...      0
    2      If only to avoid making this type of film in t...      0
    3      This film was probably inspired by Godard's Ma...      0
    4      Oh, brother...after hearing about this ridicul...      0
    ...                                                  ...    ...
    49995  Just got around to seeing Monster Man yesterda...      1
    49996  I got this as part of a competition prize. I w...      1
    49997  I got Monster Man in a box set of three films ...      1
    49998  Five minutes in, i started to feel how naff th...      1
    49999  I caught this movie on the Sci-Fi channel rece...      1
    \end{lstlisting}

  \item After the preprocessing:
    \begin{lstlisting}
    0      rented curious yellow video store controversy ...      0
    1      curious yellow risible pretentious steaming pi...      0
    2      avoid making type film future film interesting...      0
    3      film probably inspired godard masculin feminin...      0
    4      oh brother hearing ridiculous film umpteen yea...      0
    ...                                                  ...    ...
    49995  got around seeing monster man yesterday long w...      1
    49996  got part competition prize watched really expe...      1
    49997  got monster man box set three film mainly want...      1
    49998  five minute started feel naff looking got comp...      1
    49999  caught movie sci fi channel recently actually ...      1
    \end{lstlisting}
\end{itemize}

\subsection{Model architectures}

\subsubsection{LSTM model}
% Keras provides the necessary components to build a LSTM network.
The proposed model employs a neural network architecture with three core layers, combining embedding, recurrent processing, and final classification.
We will now see the model and go through each one of its layers:
\begin{lstlisting}[language=python]
model = Sequential()
model.add(Embedding(input_dim=TOKENIZER_MAX_WORDS, output_dim=EMBED_DIM))
model.add(LSTM(LSTM_OUT, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation="sigmoid"))
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
\end{lstlisting}

\paragraph{Embedding layer}
This layer's purpose is to convert integer-encoded word indices into dense vector representations.
The layer learns semantic relationships between words during training.
Instead of having a long, sparse vector, each word is mapped to a dense vector of fixed size.
These vectors are not binary, they contain real numbers that the model learns during training.
Embeddings are powerful because they use vector spaces, so they can capture intricate relationships between words.
For example, in a well trained embedding space, the vector for "king" minus "man" plus "woman" will point to queen.
Embeddings provide a way for the models to understand and relate to the data in a more sophisticated way compared to, for example, one hot encoding \cite{mediumDoesEmbedding}.

\paragraph{LSTM layer}
We have already seen in \ref{sec:lstm} how an LSTM works. 
We know that the LSTM layer processes sequential information while maintaining memory of long-range dependencies.
We add the LSTM layer in the network together with a dropout and recurrent dropout.
Dropout is a regularization technique (a process that converts the answer of a problem to a simpler one) for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data.
Dropout refers to randomly "dropping out", or omitting, units (both hidden and visible) during the training process of a neural network.
Dropout is the equivalent of training several smaller networks on the same task.
The final model is like an ensemble of smaller networks, reducing variance and providing more robust predictions.
Dropout applies dropout to the non-recurrent connections of the LSTM layer.
Recurrent dropout applies dropout to the recurrent connections (hidden statet transitions) within the LSTM \cite{wikipediaDilutionneural}.

\paragraph{Dense classification layer}
This layer serves as the final classification step.
The dense layer is a fully connected layer that takes the output from the LSTM and maps it to a single scalar value.
The LSTM typically returns a sequence of hidden states or a final hidden state.
This dense layer "summarizes" that information into a single value.
The sigmoid activation function squashes the output into the range $[0,1]$, interpreting it as a probability of the sentiment.

\paragraph{Model compilation}
The model compilation step configures the model for training by defining how it learns (the optimizer), what to optimize for (loss function) and how to evaluate the performance (evaluation metric).
The optimizer is Adam (Adaptive Moment Estimation), an advanced gradient descent alogrithm that adapts learning rates during training.
It combines the benefits of momentum (accelerates updates in consistent gradient directions) and adaptive leraning rates (adjusts rates for each parameter individually).
The loss function is the binary cross-entropy.
The purpose of the loss function is to measure the difference between predicted probabilities and true labels.
It quantifies how well the model performs.
The evaluation metric is the accuracy (the percentage of correct predictions), I didn't choose another metric like F1-score because the dataset is balanced.

\subsubsection{Bidirectional LSTM model}

The bidirectional LSTM model is basically identical to the LSTM network, but it adds a bidirectional layer.
The main difference is that while the standard LSTM processes the sequence in one direction, so it can only capture the past context, while the bidirectional LSTM processes the sequence in both directions, so it can capture both past and future context.
The bidirectional LSTM is more complex, with roughly double the parameters due to the two LSTMs.

\begin{lstlisting}[language=python]
model = Sequential()
model.add(Embedding(input_dim=TOKENIZER_MAX_WORDS, output_dim=EMBED_DIM))
model.add(Bidirectional(LSTM(LSTM_OUT, dropout=0.2, recurrent_dropout=0.2)))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer="adam", loss='binary_crossentropy', metrics=['accuracy'])
\end{lstlisting}

\subsection{Hyperparameters selection}

The hyperparameters for the network and the rationale behind them are:
\begin{itemize}
  \item \verb|TOKENIZER_MAX_WORDS|: 10000. Since adults that are native english speakers tend to have a vocabulary of 15000 to 30000 words I expect to be able to extract the positive/negative sentiment with two thirds of the tipical vocabulary \cite{babbelDoesYour}.
\end{itemize}

% VALIDATION_SPLIT = 0.2
% K_FOLDS = 5
% TOKENIZER_MAX_WORDS = 10000
% EPOCHS = 5
% EMBED_DIM = 128
% LSTM_OUT = 64

\subsection{Model training}
The models have been trained 

\bibliographystyle{unsrt}
\bibliography{references.bib}

\end{document}
