\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\title{Sentiment analysis su dataset IMDB}

\author{
 Matteo Galiazzo \\
  Dipartimento di Informatica - Scienza e Ingegneria\\
  Università di Bologna\\
  \texttt{matteo.galiazzo@studio.unibo.it} \\
  %% examples of more authors
%    \And
%  Zixuan Lu \\
%   School of Coumputing and Information\\
%   University of Pittsburgh\\
%   Pittsburgh, PA 15213 \\
%   \texttt{ZIL50@pitt.edu} \\
%   \And
%  Yuchen Lu \\
%   School of Coumputing and Information\\
%   University of Pittsburgh\\
%   Pittsburgh, PA 15213 \\
%   \texttt{yul217@pitt.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle
\begin{abstract}
TODO
\end{abstract}

% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}

This project presents an implementation of sentiment analysis on the IMDB movie reviews dataset using deep learning approaches.
The project explores two different Long Short-Term Memory (LSTM) architectures: a standard LSTM and a bidirectional LSTM model.
The implementation leverages TensorFlow and Keras for model development and training, with the goal of accurately classifying movie reviews as either positive or negative.

\subsection{Recurrent Neural Networks}

The problem of using a MLP or CNN to process text is the fact that we have a fixed input.
This means that if we use one of this architectures we are constrained by a fixed window for our input.
For solving this problem scholars introduced Recurrent Neural Networks (RNNs).
In a RNN we have a sequence of words $x_1, ..., x_n$.
To process this sequence we inject $x_1$ in the hidden layer and output $o_1$.
The information of $x_2$ is processed together with the output of the previous computation.
So in principle when you compute the output of $x_n$ you take into consideration all the previous output.
This way the sequence length is independent from the network structure.

Two very common memory cells for RNNs are Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU).
RNNs have a problem. They can only see the past of the sequence.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{img/rnn_unfolded.png}
  \caption{A compressed (left) and unfolded (right) RNN}
  \label{fig:rnn_unfolded}
\end{figure}

Abstractly speaking, an RNN is a function $f_{\theta}$ of type $(x_t, h_t) \rightarrow (y_t, h_{t+1})$, where:
\begin{itemize}
  \item $x_t$: input vector.
  \item $h_t$: hidden vector.
  \item $y_t$: output vector.
  \item $\theta$: neural network parameters.
\end{itemize}

It's a neural netowrk that maps an input $x_t$ into an output $y_t$, with the hidden vector $h_t$ playing the role of "memory", a partial record of all previous input-output pairs.
At each step, it transforms input to an output, and modifies its "memory" to help it to better perform future processing.

The figure \ref{fig:rnn_unfolded} may be misleading to many because practical neural netowrk topologies are frequently organized in "layers" and the drawing gives that appearance.
However, what appears to be layers are, in fact, different steps in time, "unfolded" to produce the appearance of layers.

We can combine two RNNs with one which gets an inverted input to see both the past and the future of the sequence.

We can combine two RNNs, one processing the input sequence in one direction, and the other one in the opposite direction
This is called a bidirectional RNN, and it's structured as follows:
\begin{itemize}
  \item The forward RNN processes in one direction: $f_{\theta}(x_0, h_0) = (y_0, h_1), f_{\theta}(x_1, h_1) = (y_1, h_2), ...$
  \item The backward RNN processes in the opposite direction:
  
  ${f'}_{\theta '}(x_N, {h'}_N) = ({y'}_N, {h'}_{N-1}), {f'}_{\theta '}(x_{N-1}, {h'}_{N-1}) = ({y'}_{N-1},{h'}_{N-2}), ...$
\end{itemize}

The two output sequences are then concatenated to give the total output: $((y_0, {y'}_0), (y_1, {y'}_1), ..., (y_N, {y'}_N))$.
Bidirectional RNN allows the model to process a token both in the context of what came before it and what came after it.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{img/rnn_bidirectional.png}
  \caption{An unfolded bidirectional RNN}
  \label{fig:rnn_bidirectional}
\end{figure}

\section{Technical implementation}

\subsection{Dataset}

The IMDB dataset is a widely used benchmark for sentiment analysis tasks.
It contains 50000 movie reviews from IMDB, allowing no more than 30 reviews per movie.
The dataset contains an even number of positive and negative reviews, so randomly guessing yields 50\% accuracy.
Also, neutral reviews are not included in the dataset \cite{imdb_dataset_stanfordnlp}.

\subsection{Data preprocessing}

The implementation includes several preprocessing steps.
The first one is data preprocessing.
Data preprocessing is important because raw textual data often contains noise, suck as typographical errors, inconsistent formatting and irrelevant symbols.
Preprocessing techniques aim to transform raw text data into a clean format that facilitates effective feature extraction and model training.

% TODO: explain what is NLTK, explain what are stopwords and what's lemmatization

In this case I cleaned the dataset by removing: links, html tags, double spaces, punctuation and stopwords.
The stopwords were removed by using NLTK (Natural Language ToolKit).
I also removed stopwords and lemmatized the dataset.

The entire dataset was finally also converted to lowercase.



% \section{Task description and data construction}
% \label{sec:headings}
% We are provided with five datasets from Kaggle: Sales train, Sale test, items, item categories and shops. In the Sales train dataset, it provides the information about the sales’ number of an item in a shop within a day. In the Sales test dataset, it provides the shop id and item id which are the items and shops we need to predict. In the other three datasets, we can get the information about item’s name and its category, and the shops’ name.
% \paragraph{Task modeling.}
% We approach this task as a regression problem. For every item and shop pair, we need to predict its next month sales(a number).
% \paragraph{Construct train and test data.}
% In the Sales train dataset, it only provides the sale within one day, but we need to predict the sale of next month. So we sum the day's sale into month's sale group by item, shop, date(within a month).
% In the Sales train dataset, it only contains two columns(item id and shop id). Because we need to provide the sales of next month, we add a date column for it, which stand for the date information of next month.

% \subsection{Headings: second level}
% \lipsum[5]
% \begin{equation}
% \xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
% \end{equation}

% \subsubsection{Headings: third level}
% \lipsum[6]

% \paragraph{Paragraph}
% \lipsum[7]

% \section{Examples of citations, figures, tables, references}
% \label{sec:others}
% \lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations
% appropriate for use in inline text.  For example,
% \begin{verbatim}
%    \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}

% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}


% \subsection{Figures}
% \lipsum[10] 
% See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
% \lipsum[11] 

% \begin{figure}
%   \centering
%   \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
%   \label{fig:fig1}
% \end{figure}

% \begin{figure} % picture
%     \centering
%     \includegraphics{test.png}
% \end{figure}

% \subsection{Tables}
% \lipsum[12]
% See awesome Table~\ref{tab:table}.

% \begin{table}
%  \caption{Sample table title}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
%   \label{tab:table}
% \end{table}

% \subsection{Lists}
% \begin{itemize}
% \item Lorem ipsum dolor sit amet
% \item consectetur adipiscing elit. 
% \item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
% \end{itemize}


\bibliographystyle{unsrt}

% Remove comment to use the external .bib file (using bibtex).
\bibliography{references.bib}
% and comment out the ``thebibliography'' section.


% %%% Comment out this section when you \bibliography{references} is enabled.
% \begin{thebibliography}{1}

% \bibitem{kour2014real}
% George Kour and Raid Saabne.
% \newblock Real-time segmentation of on-line handwritten arabic script.
% \newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
%   International Conference on}, pages 417--422. IEEE, 2014.

% \bibitem{kour2014fast}
% George Kour and Raid Saabne.
% \newblock Fast classification of handwritten on-line arabic characters.
% \newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
%   International Conference of}, pages 312--318. IEEE, 2014.

% \bibitem{hadash2018estimate}
% Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
%   Jacovi.
% \newblock Estimate and replace: A novel approach to integrating deep neural
%   networks with existing applications.
% \newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
